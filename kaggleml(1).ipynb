{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b529be",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T07:57:55.509891Z",
     "iopub.status.busy": "2022-12-07T07:57:55.509022Z",
     "iopub.status.idle": "2022-12-07T07:57:55.519626Z",
     "shell.execute_reply": "2022-12-07T07:57:55.518777Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.040635,
     "end_time": "2022-12-07T07:57:55.521641",
     "exception": false,
     "start_time": "2022-12-07T07:57:55.481006",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23199897",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:57:55.557008Z",
     "iopub.status.busy": "2022-12-07T07:57:55.556322Z",
     "iopub.status.idle": "2022-12-07T07:57:55.753065Z",
     "shell.execute_reply": "2022-12-07T07:57:55.752011Z"
    },
    "papermill": {
     "duration": 0.216469,
     "end_time": "2022-12-07T07:57:55.755554",
     "exception": false,
     "start_time": "2022-12-07T07:57:55.539085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset/vuong_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df17f96c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:57:55.790884Z",
     "iopub.status.busy": "2022-12-07T07:57:55.790574Z",
     "iopub.status.idle": "2022-12-07T07:57:55.795558Z",
     "shell.execute_reply": "2022-12-07T07:57:55.794712Z"
    },
    "papermill": {
     "duration": 0.024915,
     "end_time": "2022-12-07T07:57:55.797512",
     "exception": false,
     "start_time": "2022-12-07T07:57:55.772597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df['label'] = df['label'].replace(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e58a496",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:57:55.832585Z",
     "iopub.status.busy": "2022-12-07T07:57:55.831787Z",
     "iopub.status.idle": "2022-12-07T07:57:55.854149Z",
     "shell.execute_reply": "2022-12-07T07:57:55.853128Z"
    },
    "papermill": {
     "duration": 0.042422,
     "end_time": "2022-12-07T07:57:55.856687",
     "exception": false,
     "start_time": "2022-12-07T07:57:55.814265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       Unnamed: 0  len                                            Comment  \\\n0               0   13  xôi dẻo đồ_ăn đậm vị hộp xôi được lót lá trông...   \n1               1   53  gọi ship 1 xuất cari gà bánh naan và 3 miếng g...   \n2               2   48  thời_tiết lạnh như này cả nhà rủ nhau đến lega...   \n3               3   59  em có đọc review thấy mng bảo trà sữa nướng đề...   \n4               4   93  đồ_ăn rất ngon nhà_hàng cũng rất đẹp tất_cả đề...   \n...           ...  ...                                                ...   \n12990       12995  159  để tui gthieu cho anh_em hàng bún đậu \\ chỉ mu...   \n12991       12996    6                         đồ_ăn ổn giá_cả ko rẻ lắm    \n12992       12997   53  lâu rồi mới quay lại quán có thêm nhiều món mớ...   \n12993       12998   63  có_lẽ do khẩu_vị mỗi người mình thì ăn thấy cũ...   \n12994       12999   81  lần đầu thử đồ_ăn ấn nói_chung ngọt và cay emo...   \n\n       Rating  \n0         1.0  \n1         0.0  \n2         1.0  \n3         0.0  \n4         1.0  \n...       ...  \n12990     0.0  \n12991     0.0  \n12992     0.0  \n12993     0.0  \n12994     1.0  \n\n[12995 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>len</th>\n      <th>Comment</th>\n      <th>Rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>13</td>\n      <td>xôi dẻo đồ_ăn đậm vị hộp xôi được lót lá trông...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>53</td>\n      <td>gọi ship 1 xuất cari gà bánh naan và 3 miếng g...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>48</td>\n      <td>thời_tiết lạnh như này cả nhà rủ nhau đến lega...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>59</td>\n      <td>em có đọc review thấy mng bảo trà sữa nướng đề...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>93</td>\n      <td>đồ_ăn rất ngon nhà_hàng cũng rất đẹp tất_cả đề...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12990</th>\n      <td>12995</td>\n      <td>159</td>\n      <td>để tui gthieu cho anh_em hàng bún đậu \\ chỉ mu...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>12991</th>\n      <td>12996</td>\n      <td>6</td>\n      <td>đồ_ăn ổn giá_cả ko rẻ lắm</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>12992</th>\n      <td>12997</td>\n      <td>53</td>\n      <td>lâu rồi mới quay lại quán có thêm nhiều món mớ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>12993</th>\n      <td>12998</td>\n      <td>63</td>\n      <td>có_lẽ do khẩu_vị mỗi người mình thì ăn thấy cũ...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>12994</th>\n      <td>12999</td>\n      <td>81</td>\n      <td>lần đầu thử đồ_ăn ấn nói_chung ngọt và cay emo...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>12995 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94828a87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:57:55.894330Z",
     "iopub.status.busy": "2022-12-07T07:57:55.892807Z",
     "iopub.status.idle": "2022-12-07T07:57:55.898122Z",
     "shell.execute_reply": "2022-12-07T07:57:55.897172Z"
    },
    "papermill": {
     "duration": 0.025542,
     "end_time": "2022-12-07T07:57:55.900115",
     "exception": false,
     "start_time": "2022-12-07T07:57:55.874573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = df[['RevId','Comment','label','Rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa5aebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:57:55.936214Z",
     "iopub.status.busy": "2022-12-07T07:57:55.934732Z",
     "iopub.status.idle": "2022-12-07T07:57:55.947834Z",
     "shell.execute_reply": "2022-12-07T07:57:55.946829Z"
    },
    "papermill": {
     "duration": 0.033195,
     "end_time": "2022-12-07T07:57:55.950035",
     "exception": false,
     "start_time": "2022-12-07T07:57:55.916840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "False    12995\nName: Comment, dtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Comment'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db852b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:57:55.984796Z",
     "iopub.status.busy": "2022-12-07T07:57:55.984523Z",
     "iopub.status.idle": "2022-12-07T07:57:55.988640Z",
     "shell.execute_reply": "2022-12-07T07:57:55.987508Z"
    },
    "papermill": {
     "duration": 0.023957,
     "end_time": "2022-12-07T07:57:55.990695",
     "exception": false,
     "start_time": "2022-12-07T07:57:55.966738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df['len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c6d205",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:57:56.026089Z",
     "iopub.status.busy": "2022-12-07T07:57:56.025127Z",
     "iopub.status.idle": "2022-12-07T07:57:56.030137Z",
     "shell.execute_reply": "2022-12-07T07:57:56.029361Z"
    },
    "papermill": {
     "duration": 0.02476,
     "end_time": "2022-12-07T07:57:56.032086",
     "exception": false,
     "start_time": "2022-12-07T07:57:56.007326",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list_len = df['len'].to_list()\n",
    "# list_text = df['Comment'].to_list()\n",
    "# list_label = df['label'].to_list()\n",
    "\n",
    "# for i in range(len(list_len)):\n",
    "#     if list_len[i] >= 256:\n",
    "#         print(list_text[i])\n",
    "#         tmp = list_text[i].split()\n",
    "#         list_text[i] =  ' '.join(tmp[0:210]) + ' ' + ' '.join(tmp[-46:])\n",
    "#         print()\n",
    "#         print(list_text[i])\n",
    "#         print(list_label[i])\n",
    "#         print(\"hihihi\")\n",
    "\n",
    "# df['Comment'] = list_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "658a9a1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:57:56.070184Z",
     "iopub.status.busy": "2022-12-07T07:57:56.068626Z",
     "iopub.status.idle": "2022-12-07T07:57:56.083487Z",
     "shell.execute_reply": "2022-12-07T07:57:56.082459Z"
    },
    "papermill": {
     "duration": 0.035165,
     "end_time": "2022-12-07T07:57:56.085492",
     "exception": false,
     "start_time": "2022-12-07T07:57:56.050327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(12995, 4)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70597592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:57:56.121557Z",
     "iopub.status.busy": "2022-12-07T07:57:56.120707Z",
     "iopub.status.idle": "2022-12-07T07:57:56.126208Z",
     "shell.execute_reply": "2022-12-07T07:57:56.125341Z"
    },
    "papermill": {
     "duration": 0.025609,
     "end_time": "2022-12-07T07:57:56.128563",
     "exception": false,
     "start_time": "2022-12-07T07:57:56.102954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Rating'] = df['Rating'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03bfbf8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:57:56.164942Z",
     "iopub.status.busy": "2022-12-07T07:57:56.164673Z",
     "iopub.status.idle": "2022-12-07T07:57:57.058839Z",
     "shell.execute_reply": "2022-12-07T07:57:57.057547Z"
    },
    "papermill": {
     "duration": 0.914726,
     "end_time": "2022-12-07T07:57:57.061289",
     "exception": false,
     "start_time": "2022-12-07T07:57:56.146563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((11695, 4), (1300, 4))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7949, 4) (3746, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "df_train_x = df_train[df_train['Rating'] == 1]\n",
    "df_train_y = df_train[df_train['Rating'] == 0]\n",
    "print(df_train_x.shape, df_train_y.shape)\n",
    "df_train_x = df_train_x.sample(7300)\n",
    "df_train_y= df_train_y.sample(1200)\n",
    "\n",
    "df_train = pd.concat([df_train_x, df_train[df_train['Rating'] == 0], df_train_y],axis = 0)\n",
    "\n",
    "df_train = shuffle(df_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6212ac0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:57:57.150678Z",
     "iopub.status.busy": "2022-12-07T07:57:57.150371Z",
     "iopub.status.idle": "2022-12-07T07:57:57.154619Z",
     "shell.execute_reply": "2022-12-07T07:57:57.153665Z"
    },
    "papermill": {
     "duration": 0.025462,
     "end_time": "2022-12-07T07:57:57.156690",
     "exception": false,
     "start_time": "2022-12-07T07:57:57.131228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_test = df_test[['Comment','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93de759b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:57:57.193081Z",
     "iopub.status.busy": "2022-12-07T07:57:57.192092Z",
     "iopub.status.idle": "2022-12-07T07:57:57.201702Z",
     "shell.execute_reply": "2022-12-07T07:57:57.200751Z"
    },
    "papermill": {
     "duration": 0.029672,
     "end_time": "2022-12-07T07:57:57.203660",
     "exception": false,
     "start_time": "2022-12-07T07:57:57.173988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1    7300\n0    4946\nName: Rating, dtype: int64"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e185fa05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:57:57.267111Z",
     "iopub.status.busy": "2022-12-07T07:57:57.266632Z",
     "iopub.status.idle": "2022-12-07T07:58:06.405157Z",
     "shell.execute_reply": "2022-12-07T07:58:06.404143Z"
    },
    "papermill": {
     "duration": 9.179001,
     "end_time": "2022-12-07T07:58:06.407622",
     "exception": false,
     "start_time": "2022-12-07T07:57:57.228621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PythonFile\\miniconda3\\envs\\Kaggle\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.svm import SVC\n",
    "from joblib import dump\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import regularizers, constraints\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3836729b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:06.468491Z",
     "iopub.status.busy": "2022-12-07T07:58:06.468091Z",
     "iopub.status.idle": "2022-12-07T07:58:06.483175Z",
     "shell.execute_reply": "2022-12-07T07:58:06.480193Z"
    },
    "papermill": {
     "duration": 0.051745,
     "end_time": "2022-12-07T07:58:06.486883",
     "exception": false,
     "start_time": "2022-12-07T07:58:06.435138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4c17b35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:06.558851Z",
     "iopub.status.busy": "2022-12-07T07:58:06.558347Z",
     "iopub.status.idle": "2022-12-07T07:58:06.654003Z",
     "shell.execute_reply": "2022-12-07T07:58:06.652986Z"
    },
    "papermill": {
     "duration": 0.126578,
     "end_time": "2022-12-07T07:58:06.656631",
     "exception": false,
     "start_time": "2022-12-07T07:58:06.530053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def load_bert():\n",
    "    v_phobert = AutoModel.from_pretrained(\"vinai/phobert-base\").to(device)\n",
    "    v_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "    return v_phobert, v_tokenizer\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6ab4e8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:06.716535Z",
     "iopub.status.busy": "2022-12-07T07:58:06.716106Z",
     "iopub.status.idle": "2022-12-07T07:58:35.390253Z",
     "shell.execute_reply": "2022-12-07T07:58:35.388873Z"
    },
    "papermill": {
     "duration": 28.705909,
     "end_time": "2022-12-07T07:58:35.393167",
     "exception": false,
     "start_time": "2022-12-07T07:58:06.687258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "v_phobert = AutoModel.from_pretrained(\"vinai/phobert-base\").to(device)\n",
    "v_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc92a652",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:35.433580Z",
     "iopub.status.busy": "2022-12-07T07:58:35.432036Z",
     "iopub.status.idle": "2022-12-07T07:58:35.437233Z",
     "shell.execute_reply": "2022-12-07T07:58:35.436310Z"
    },
    "papermill": {
     "duration": 0.026279,
     "end_time": "2022-12-07T07:58:35.439219",
     "exception": false,
     "start_time": "2022-12-07T07:58:35.412940",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# v_phobert.embeddings.position_embeddings = nn.Embedding(1024, 768, padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aedc291e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:35.476244Z",
     "iopub.status.busy": "2022-12-07T07:58:35.475975Z",
     "iopub.status.idle": "2022-12-07T07:58:35.486535Z",
     "shell.execute_reply": "2022-12-07T07:58:35.485701Z"
    },
    "papermill": {
     "duration": 0.031286,
     "end_time": "2022-12-07T07:58:35.488266",
     "exception": false,
     "start_time": "2022-12-07T07:58:35.456980",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(64001, 768, padding_idx=1)\n    (position_embeddings): Embedding(258, 768, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_phobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b9ab69b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:35.526465Z",
     "iopub.status.busy": "2022-12-07T07:58:35.526181Z",
     "iopub.status.idle": "2022-12-07T07:58:35.531878Z",
     "shell.execute_reply": "2022-12-07T07:58:35.530816Z"
    },
    "papermill": {
     "duration": 0.026866,
     "end_time": "2022-12-07T07:58:35.533820",
     "exception": false,
     "start_time": "2022-12-07T07:58:35.506954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizer(name_or_path='vinai/phobert-base', vocab_size=64000, model_max_len=256, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'})"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4136db58",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:35.571848Z",
     "iopub.status.busy": "2022-12-07T07:58:35.570946Z",
     "iopub.status.idle": "2022-12-07T07:58:35.577929Z",
     "shell.execute_reply": "2022-12-07T07:58:35.576843Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.028598,
     "end_time": "2022-12-07T07:58:35.580649",
     "exception": false,
     "start_time": "2022-12-07T07:58:35.552051",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD phoBERT DONE\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 256\n",
    "print(\"LOAD phoBERT DONE\")\n",
    "\n",
    "def get_emb_vector(input_ids):\n",
    "    input_ids  = torch.tensor([input_ids]).to(torch.long)\n",
    "    with torch.no_grad():\n",
    "        features = phobert(input_ids.to(torch.device(\"cuda:0\")))\n",
    "    #print(features)\n",
    "    emb_vecs = features[0].cpu().numpy()[0]#[1:-1]\n",
    "    #print(emb_vecs)\n",
    "    return emb_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cea66142",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:35.619996Z",
     "iopub.status.busy": "2022-12-07T07:58:35.619218Z",
     "iopub.status.idle": "2022-12-07T07:58:35.624185Z",
     "shell.execute_reply": "2022-12-07T07:58:35.623379Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026361,
     "end_time": "2022-12-07T07:58:35.626157",
     "exception": false,
     "start_time": "2022-12-07T07:58:35.599796",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text2ids(text):\n",
    "    # print(tokenizer.encode(\"<pad> nhà <pad>\"))\n",
    "    tkz = tokenizer.encode(text)\n",
    "    return tkz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4b033",
   "metadata": {
    "papermill": {
     "duration": 0.018182,
     "end_time": "2022-12-07T07:58:35.663152",
     "exception": false,
     "start_time": "2022-12-07T07:58:35.644970",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cd37f20",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:35.701522Z",
     "iopub.status.busy": "2022-12-07T07:58:35.700579Z",
     "iopub.status.idle": "2022-12-07T07:58:35.713662Z",
     "shell.execute_reply": "2022-12-07T07:58:35.712727Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.03417,
     "end_time": "2022-12-07T07:58:35.715668",
     "exception": false,
     "start_time": "2022-12-07T07:58:35.681498",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.utils.data_utils import Sequence\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, ids, labels, batch_size=16,  max_seq_len=256, feature_len=768, n_classes=18, shuffle=True):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.feature_len = feature_len\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.ids = ids\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ids))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, idx_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, self.max_seq_len, self.feature_len))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        # Generate data\n",
    "        for i, idx in enumerate(idx_temp):\n",
    "            X[i,] = get_emb_vector(self.ids[idx])\n",
    "            # Store class\n",
    "            y[i] = self.labels[idx]\n",
    "        # X = X[:,:,:,np.newaxis]\n",
    "        # y = to_categorical(y, num_classes=self.n_classes)\n",
    "        # print(y.shape)\n",
    "        # exit()\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1e9aba6",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:35.754572Z",
     "iopub.status.busy": "2022-12-07T07:58:35.753697Z",
     "iopub.status.idle": "2022-12-07T07:58:35.758213Z",
     "shell.execute_reply": "2022-12-07T07:58:35.757380Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025871,
     "end_time": "2022-12-07T07:58:35.760067",
     "exception": false,
     "start_time": "2022-12-07T07:58:35.734196",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_generator = DataGenerator(ids_train, y_train, batch_size=8, n_classes=len(classes))\n",
    "# valid_generator = DataGenerator(ids_test, y_test, batch_size=8, n_classes=len(classes), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f154a",
   "metadata": {
    "papermill": {
     "duration": 0.018102,
     "end_time": "2022-12-07T07:58:35.796563",
     "exception": false,
     "start_time": "2022-12-07T07:58:35.778461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Finetuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28b1d320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:35.835458Z",
     "iopub.status.busy": "2022-12-07T07:58:35.833993Z",
     "iopub.status.idle": "2022-12-07T07:58:35.838836Z",
     "shell.execute_reply": "2022-12-07T07:58:35.837975Z"
    },
    "papermill": {
     "duration": 0.02591,
     "end_time": "2022-12-07T07:58:35.840761",
     "exception": false,
     "start_time": "2022-12-07T07:58:35.814851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def standardize_data(row):\n",
    "#     # Xóa dấu chấm, phẩy, hỏi ở cuối câu\n",
    "#     row = re.sub(r\"[\\.,\\?]+$-\", \"\", row)\n",
    "#     # Xóa tất cả dấu chấm, phẩy, chấm phẩy, chấm thang, ... trong câu\n",
    "#     row = row.replace(\",\", \" \").replace(\".\", \" \") \\\n",
    "#         .replace(\";\", \" \").replace(\"“\", \" \") \\\n",
    "#         .replace(\":\", \" \").replace(\"”\", \" \") \\\n",
    "#         .replace('\"', \" \").replace(\"'\", \" \") \\\n",
    "#         .replace(\"!\", \" \").replace(\"?\", \" \") \\\n",
    "#         .replace(\"-\", \" \").replace(\"?\", \" \")\n",
    "#     return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d5979d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:35.878934Z",
     "iopub.status.busy": "2022-12-07T07:58:35.878672Z",
     "iopub.status.idle": "2022-12-07T07:58:35.893376Z",
     "shell.execute_reply": "2022-12-07T07:58:35.892087Z"
    },
    "papermill": {
     "duration": 0.036374,
     "end_time": "2022-12-07T07:58:35.895330",
     "exception": false,
     "start_time": "2022-12-07T07:58:35.858956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class Bert_fine_tuning(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert_fine_tuning, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"vinai/phobert-base\").to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=768, out_features = 500, bias=True).to(device),\n",
    "            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True).to(device),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(in_features=500, out_features = 2, bias=True).to(device)\n",
    "        )\n",
    "        self.criterior = nn.CrossEntropyLoss()\n",
    "        # self.criterior = nn.BCEWithLogitsLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        param_optimizer = list(self.bert.named_parameters())\n",
    "        self.optimizer =optim.AdamW(\n",
    "#             [{'params': self.bert.parameters(), 'lr': 8e-5}, \n",
    "#             {'params': self.classifier.parameters(), 'lr': 1e-3}],\n",
    "            [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "            {'params': self.classifier.parameters(), 'lr': 1e-3}],\n",
    "            lr = 2e-5\n",
    "#              momentum=0.9\n",
    "        )\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "      \n",
    "    \n",
    "    def tokenize(self,v_text, label):\n",
    "        v_tokenized = []\n",
    "        max_len = 256 \n",
    "        for i_text in v_text:\n",
    "#             i_text = standardize_data(i_text)\n",
    "            line = self.tokenizer.encode(i_text, max_length = 256)\n",
    "            v_tokenized.append(line)\n",
    "\n",
    "        padded = numpy.array([i + [1] * (max_len - len(i)) for i in v_tokenized])\n",
    "        print('padded:', padded[0])\n",
    "        print('len padded:', padded.shape)\n",
    "        attention_mask = numpy.where(padded == 1, 0, 1)\n",
    "        print('attention mask:', attention_mask[0])\n",
    "        padded = torch.tensor(padded).to(torch.long)\n",
    "        print(\"Padd = \",padded.size())\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "        \n",
    "        target = torch.tensor(label) \n",
    "        print(target)\n",
    "        data_tensor = TensorDataset(padded, target) \n",
    "        train_loader = DataLoader(dataset = data_tensor, batch_size = 32)\n",
    "        attention_mask = DataLoader(attention_mask, batch_size = 32)\n",
    "        \n",
    "        return train_loader, attention_mask\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "             return_dict=False\n",
    "        )\n",
    "#         self.drop = nn.Dropout(p=0.2)\n",
    "#         output = self.drop(pooled_output)\n",
    "        output = self.classifier(output)\n",
    "        return output\n",
    "#         return self.sigmoid(output)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bdd4709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:35.933406Z",
     "iopub.status.busy": "2022-12-07T07:58:35.932525Z",
     "iopub.status.idle": "2022-12-07T07:58:35.944386Z",
     "shell.execute_reply": "2022-12-07T07:58:35.943566Z"
    },
    "papermill": {
     "duration": 0.032794,
     "end_time": "2022-12-07T07:58:35.946293",
     "exception": false,
     "start_time": "2022-12-07T07:58:35.913499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def train_model(bert, save_path, df_train, df_val, num_epochs=1):\n",
    "        text_train = df_train['Comment'].to_list()\n",
    "        label_train = df_train['Rating'].to_list()\n",
    "        text_test = df_test['Comment'].to_list()\n",
    "        label_test = df_test['Rating'].to_list()\n",
    "        \n",
    "        Data_train, attention_train = bert.tokenize(text_train, label_train)\n",
    "        Data_test, attention_test = bert.tokenize(text_test, label_test)\n",
    "        \n",
    "        dataloader_dict = {\n",
    "            'train' : Data_train,\n",
    "            'test' : Data_test\n",
    "        }\n",
    "\n",
    "        list_tmp = []\n",
    "        \n",
    "        for epoch in range(num_epochs + 1):\n",
    "            if epoch == 0:\n",
    "                print()\n",
    "                continue\n",
    "            print(\"Epoch {}/{} \".format(epoch, num_epochs))\n",
    "\n",
    "            for phase in ['train', 'test']:\n",
    "                if phase == 'train':\n",
    "                    bert.train()\n",
    "                    attention = attention_train\n",
    "                else:\n",
    "                    bert.eval()\n",
    "                    attention = attention_test\n",
    "\n",
    "                epoch_loss = 0.0\n",
    "                epoch_corrects = 0\n",
    "\n",
    "                for (inputs, labels), i_attention in tqdm(zip(dataloader_dict[phase], attention)):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    i_attention = i_attention.to(device)\n",
    "\n",
    "                    bert.optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = bert(inputs,i_attention)\n",
    "\n",
    "                        loss = bert.criterior(outputs, labels)\n",
    "\n",
    "                        list_tmp.append(outputs)\n",
    "                        print(outputs)\n",
    "\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                        del outputs\n",
    "                        torch.cuda.empty_cache()\n",
    "                        gc.collect()\n",
    "\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            bert.optimizer.step()\n",
    "\n",
    "                        epoch_loss += loss.item() * inputs.size(0)\n",
    "                        epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                    # with torch.no_grad():\n",
    "                    #     outputs = bert(inputs, i_attention)\n",
    "                    #     loss = bert.criterior(outputs, labels)\n",
    "                    #\n",
    "                    #     list_tmp.append(outputs)\n",
    "                    #     print(outputs)\n",
    "                    #\n",
    "                    #     _, preds = torch.max(outputs, 1)\n",
    "                    #\n",
    "                    #     del outputs\n",
    "                    #     torch.cuda.empty_cache()\n",
    "                    #     gc.collect()\n",
    "                    #\n",
    "                    #     if phase == 'train':\n",
    "                    #         loss.backward()\n",
    "                    #         bert.optimizer.step()\n",
    "                    #\n",
    "                    #     epoch_loss += loss.item() * inputs.size(0)\n",
    "                    #     epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "\n",
    "                epoch_loss = epoch_loss / len(dataloader_dict[phase].dataset)\n",
    "                epoch_acc = epoch_corrects.double() / len(dataloader_dict[phase].dataset)\n",
    "\n",
    "                print(\"{} Loss: {:.4f}  Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
    "                torch.save(bert.bert.state_dict(), \"./bert_model{}.pth\".format(epoch))\n",
    "                torch.save(bert.state_dict(), \"./full_bert_model{}.pth\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4162c995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:35.984521Z",
     "iopub.status.busy": "2022-12-07T07:58:35.983649Z",
     "iopub.status.idle": "2022-12-07T07:58:35.987889Z",
     "shell.execute_reply": "2022-12-07T07:58:35.987081Z"
    },
    "papermill": {
     "duration": 0.025332,
     "end_time": "2022-12-07T07:58:35.989885",
     "exception": false,
     "start_time": "2022-12-07T07:58:35.964553",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dee1e9e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T07:58:36.028085Z",
     "iopub.status.busy": "2022-12-07T07:58:36.027316Z",
     "iopub.status.idle": "2022-12-07T08:19:34.868663Z",
     "shell.execute_reply": "2022-12-07T08:19:34.867139Z"
    },
    "papermill": {
     "duration": 1258.868207,
     "end_time": "2022-12-07T08:19:34.876274",
     "exception": false,
     "start_time": "2022-12-07T07:58:36.008067",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded: [    0   946  1813   819    99  3864    30    26   328 16132    28   154\n",
      "   119    99 13295    68   328  4650  1074   946  1813  4650  2809   412\n",
      "   656  1171    63  2117  1074  2122    42  1357    51    26   154    11\n",
      "   779   548    40  2804  1456  4650    54   412   354  1074    54   515\n",
      "  3950  4311  2117  2725   281     8  2804    50    28   390  4036    54\n",
      " 11685  4650  1834   203    11     2     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1]\n",
      "len padded: (12246, 256)\n",
      "attention mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Padd =  torch.Size([12246, 256])\n",
      "tensor([1, 0, 0,  ..., 1, 1, 1])\n",
      "padded: [    0   949   191     8 18130    90   328   320   509 25184  2071   287\n",
      "    99   436   213   320   509  1625 57703  4598  1625 11638   193    72\n",
      "   779    57  2827    38    45  5841   954    30   219    30    54  2710\n",
      "    17    10   195   320   509   276    68   358    18   650  2710   663\n",
      "   918   509   227    68   447    10   509    85    17    12  6602  1822\n",
      "    17  2710    10   954   376    33   517  1333   145    12  5582     8\n",
      "    17  6602  1822   898 31827  8029    99    88  1493  2774   650   118\n",
      "   260    96   297   889   509    12    23   336   320   182   336  1652\n",
      "    33    97  7742   570  6377  5516   320   509  3808   101    23     8\n",
      " 25052   957   182    44  6104   138    40   227 19570    50    10 10294\n",
      "  6773  6909     6  2060 12606   328    58   650   156  5321 49544    40\n",
      "    51  5516   156  4410    44   663   971  4410   954   971   494   650\n",
      "  5048   204   118   423 20000 51403    83   102   108   944   224   142\n",
      "  2169     8    17  1564 25052    12    18    77    68  5516 37973    28\n",
      "  4036    90   102   203    14   108   274  1325    94  1339   203    68\n",
      "    66   487   131     8    47   399    94     9    88    64    47    25\n",
      " 44221  9295  5687  9295  2542    13    17    50  4028   142   533   931\n",
      "    23   260    32   620   412   236  1532    96    30   650     2     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1]\n",
      "len padded: (1300, 256)\n",
      "attention mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Padd =  torch.Size([1300, 256])\n",
      "tensor([0, 0, 1,  ..., 1, 1, 0])\n",
      "\n",
      "Epoch 1/5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 8.00 GiB total capacity; 7.25 GiB already allocated; 0 bytes free; 7.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [30], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# from pytorch_pretrained_bert.optimization import BertAdam\u001B[39;00m\n\u001B[0;32m      3\u001B[0m b \u001B[38;5;241m=\u001B[39m  Bert_fine_tuning()\n\u001B[1;32m----> 4\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [28], line 44\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(bert, save_path, df_train, df_val, num_epochs)\u001B[0m\n\u001B[0;32m     41\u001B[0m bert\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(phase \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m---> 44\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43mi_attention\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     46\u001B[0m     loss \u001B[38;5;241m=\u001B[39m bert\u001B[38;5;241m.\u001B[39mcriterior(outputs, labels)\n\u001B[0;32m     48\u001B[0m     list_tmp\u001B[38;5;241m.\u001B[39mappend(outputs)\n",
      "File \u001B[1;32mD:\\PythonFile\\miniconda3\\envs\\Kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn [27], line 58\u001B[0m, in \u001B[0;36mBert_fine_tuning.forward\u001B[1;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_ids, attention_mask):\n\u001B[1;32m---> 58\u001B[0m         _, output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m             \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m     62\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;66;03m#         self.drop = nn.Dropout(p=0.2)\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;66;03m#         output = self.drop(pooled_output)\u001B[39;00m\n\u001B[0;32m     65\u001B[0m         output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(output)\n",
      "File \u001B[1;32mD:\\PythonFile\\miniconda3\\envs\\Kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\PythonFile\\miniconda3\\envs\\Kaggle\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:853\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    844\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m    846\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[0;32m    847\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m    848\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    851\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[0;32m    852\u001B[0m )\n\u001B[1;32m--> 853\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    854\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    855\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    856\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    857\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    859\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    861\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    863\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    864\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    865\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    866\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\PythonFile\\miniconda3\\envs\\Kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\PythonFile\\miniconda3\\envs\\Kaggle\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:527\u001B[0m, in \u001B[0;36mRobertaEncoder.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    518\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[0;32m    519\u001B[0m         create_custom_forward(layer_module),\n\u001B[0;32m    520\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    524\u001B[0m         encoder_attention_mask,\n\u001B[0;32m    525\u001B[0m     )\n\u001B[0;32m    526\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 527\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    528\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    529\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    530\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    532\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    533\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    534\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    535\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    537\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32mD:\\PythonFile\\miniconda3\\envs\\Kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\PythonFile\\miniconda3\\envs\\Kaggle\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:412\u001B[0m, in \u001B[0;36mRobertaLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    400\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    401\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    402\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    409\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m    410\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[0;32m    411\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 412\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    416\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    417\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    418\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    419\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    421\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[1;32mD:\\PythonFile\\miniconda3\\envs\\Kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\PythonFile\\miniconda3\\envs\\Kaggle\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:339\u001B[0m, in \u001B[0;36mRobertaAttention.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    331\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    337\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    338\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m--> 339\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    340\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    341\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    342\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    343\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    344\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    345\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    346\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    347\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    348\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[0;32m    349\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[1;32mD:\\PythonFile\\miniconda3\\envs\\Kaggle\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\PythonFile\\miniconda3\\envs\\Kaggle\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:259\u001B[0m, in \u001B[0;36mRobertaSelfAttention.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    256\u001B[0m         relative_position_scores_key \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meinsum(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbhrd,lrd->bhlr\u001B[39m\u001B[38;5;124m\"\u001B[39m, key_layer, positional_embedding)\n\u001B[0;32m    257\u001B[0m         attention_scores \u001B[38;5;241m=\u001B[39m attention_scores \u001B[38;5;241m+\u001B[39m relative_position_scores_query \u001B[38;5;241m+\u001B[39m relative_position_scores_key\n\u001B[1;32m--> 259\u001B[0m attention_scores \u001B[38;5;241m=\u001B[39m \u001B[43mattention_scores\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention_head_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;66;03m# Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\u001B[39;00m\n\u001B[0;32m    262\u001B[0m     attention_scores \u001B[38;5;241m=\u001B[39m attention_scores \u001B[38;5;241m+\u001B[39m attention_mask\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 8.00 GiB total capacity; 7.25 GiB already allocated; 0 bytes free; 7.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "b =  Bert_fine_tuning()\n",
    "train_model(b,\"./\", df_train, df_test, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50e400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:19:35.638034Z",
     "iopub.status.busy": "2022-12-07T08:19:35.637492Z",
     "iopub.status.idle": "2022-12-07T08:19:45.239829Z",
     "shell.execute_reply": "2022-12-07T08:19:45.236711Z"
    },
    "papermill": {
     "duration": 10.032022,
     "end_time": "2022-12-07T08:19:45.242725",
     "exception": false,
     "start_time": "2022-12-07T08:19:35.210703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = Bert_fine_tuning()\n",
    "w = torch.load('full_bert_model1.pth')\n",
    "b.load_state_dict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0103f49",
   "metadata": {
    "papermill": {
     "duration": 0.237836,
     "end_time": "2022-12-07T08:19:45.728662",
     "exception": false,
     "start_time": "2022-12-07T08:19:45.490826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b219d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:19:46.208081Z",
     "iopub.status.busy": "2022-12-07T08:19:46.207728Z",
     "iopub.status.idle": "2022-12-07T08:19:56.178015Z",
     "shell.execute_reply": "2022-12-07T08:19:56.176798Z"
    },
    "papermill": {
     "duration": 10.213504,
     "end_time": "2022-12-07T08:19:56.180221",
     "exception": false,
     "start_time": "2022-12-07T08:19:45.966717",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "text_test = df_test['Comment'].to_list()\n",
    "label_test = df_test['Rating'].to_list()\n",
    "\n",
    "epoch_loss = 0.0\n",
    "epoch_corrects = 0\n",
    "net = b\n",
    "net.eval()\n",
    "Data_test, attention_test = b.tokenize(text_test, label_test)\n",
    "attention = attention_test\n",
    "\n",
    "list_pred = np.array([])\n",
    "            \n",
    "for (inputs, labels), i_attention in tqdm(zip(Data_test, attention)):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    i_attention = i_attention.to(device)\n",
    "    \n",
    "    outputs = b(inputs,i_attention)\n",
    "        \n",
    "    loss = b.criterior(outputs,labels)      \n",
    "        \n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "#     print(preds.cpu().numpy())\n",
    "    list_pred = np.append(list_pred,preds.cpu().numpy())\n",
    "        \n",
    "    epoch_loss += loss.item() * inputs.size(0)\n",
    "    epoch_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "epoch_loss = epoch_loss / len(Data_test.dataset)\n",
    "epoch_acc = epoch_corrects.double() / len(Data_test.dataset)\n",
    "            \n",
    "print(\" Loss: {:.4f}  Acc: {:.4f}\".format( epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dad412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:19:56.713308Z",
     "iopub.status.busy": "2022-12-07T08:19:56.712941Z",
     "iopub.status.idle": "2022-12-07T08:19:56.717221Z",
     "shell.execute_reply": "2022-12-07T08:19:56.716226Z"
    },
    "papermill": {
     "duration": 0.291214,
     "end_time": "2022-12-07T08:19:56.719167",
     "exception": false,
     "start_time": "2022-12-07T08:19:56.427953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save(b.bert.state_dict(),\"./bert_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338bd54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:19:57.198478Z",
     "iopub.status.busy": "2022-12-07T08:19:57.198098Z",
     "iopub.status.idle": "2022-12-07T08:19:57.212607Z",
     "shell.execute_reply": "2022-12-07T08:19:57.211351Z"
    },
    "papermill": {
     "duration": 0.25699,
     "end_time": "2022-12-07T08:19:57.214627",
     "exception": false,
     "start_time": "2022-12-07T08:19:56.957637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# list_pred\n",
    "\n",
    "print(\"BERT Accuracy Score -> \",accuracy_score(list_pred, label_test)*100)\n",
    "\n",
    "print(classification_report(label_test,list_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8eacd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:19:57.939628Z",
     "iopub.status.busy": "2022-12-07T08:19:57.939258Z",
     "iopub.status.idle": "2022-12-07T08:19:57.944105Z",
     "shell.execute_reply": "2022-12-07T08:19:57.943146Z"
    },
    "papermill": {
     "duration": 0.312616,
     "end_time": "2022-12-07T08:19:57.946031",
     "exception": false,
     "start_time": "2022-12-07T08:19:57.633415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check 1\n",
    "\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "# ]\n",
    "# num_train_optimization_steps = int(args.epochs*len(train_df)/args.batch_size/args.accumulation_steps)\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=args.lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25979436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:19:58.498158Z",
     "iopub.status.busy": "2022-12-07T08:19:58.497789Z",
     "iopub.status.idle": "2022-12-07T08:19:58.515155Z",
     "shell.execute_reply": "2022-12-07T08:19:58.514229Z"
    },
    "papermill": {
     "duration": 0.314711,
     "end_time": "2022-12-07T08:19:58.517183",
     "exception": false,
     "start_time": "2022-12-07T08:19:58.202472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Attention Layer\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'W_regularizer': self.W_regularizer,\n",
    "            'u_regularizer': self.u_regularizer,\n",
    "            'b_regularizer': self.b_regularizer,\n",
    "            'W_constraint': self.W_constraint,\n",
    "            'u_constraint': self.u_constraint,\n",
    "            'b_constraint': self.b_constraint,\n",
    "            'bias': self.bias\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ddef9",
   "metadata": {
    "papermill": {
     "duration": 0.264781,
     "end_time": "2022-12-07T08:19:59.034783",
     "exception": false,
     "start_time": "2022-12-07T08:19:58.770002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## combine with tree-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cec4c1",
   "metadata": {
    "papermill": {
     "duration": 0.238386,
     "end_time": "2022-12-07T08:19:59.512437",
     "exception": false,
     "start_time": "2022-12-07T08:19:59.274051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6d2b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:19:59.995003Z",
     "iopub.status.busy": "2022-12-07T08:19:59.994641Z",
     "iopub.status.idle": "2022-12-07T08:20:00.005562Z",
     "shell.execute_reply": "2022-12-07T08:20:00.004703Z"
    },
    "papermill": {
     "duration": 0.254531,
     "end_time": "2022-12-07T08:20:00.007502",
     "exception": false,
     "start_time": "2022-12-07T08:19:59.752971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_bert_features(v_text):\n",
    "    global phobert, sw\n",
    "    v_tokenized = []\n",
    "    \n",
    "    max_len = 256 \n",
    "    for i_text in v_text:\n",
    "#         i_text = standardize_data(i_text)\n",
    "        line = tokenizer.encode(i_text, max_length = 256)\n",
    "        v_tokenized.append(line)\n",
    "\n",
    "    \n",
    "    # Chèn thêm số 1 vào cuối câu nếu như không đủ 256 từ\n",
    "    padded = numpy.array([i + [1] * (max_len - len(i)) for i in v_tokenized])\n",
    "    print('padded:', padded[0])\n",
    "    print('len padded:', padded.shape)\n",
    "\n",
    "    # Đánh dấu các từ thêm vào = 0 để không tính vào quá trình lấy features\n",
    "    attention_mask = numpy.where(padded == 1, 0, 1)\n",
    "    print('attention mask:', attention_mask[0])\n",
    "\n",
    "    padded = torch.tensor(padded).to(torch.long)\n",
    "    print(\"Padd = \",padded.size())\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    \n",
    "    padded = DataLoader(padded, batch_size=32)\n",
    "    attention_mask = DataLoader(attention_mask, batch_size = 32)\n",
    "    check = False\n",
    "    v_feats = None\n",
    "    for data, i_mask in zip(padded, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = phobert(input_ids=data.to(device), attention_mask = i_mask.to(device))\n",
    "            v_features = last_hidden_states[0][:, 0, :].cpu().numpy()\n",
    "            if check is False: \n",
    "                v_feats = v_features\n",
    "                check = True\n",
    "            else:\n",
    "                v_feats = np.append(v_feats, v_features, axis = 0)\n",
    "                \n",
    "        print(v_feats.shape)\n",
    "                                    \n",
    "    print(v_feats.shape)\n",
    "    return v_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dbb324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:20:00.531703Z",
     "iopub.status.busy": "2022-12-07T08:20:00.531296Z",
     "iopub.status.idle": "2022-12-07T08:21:14.204117Z",
     "shell.execute_reply": "2022-12-07T08:21:14.203043Z"
    },
    "papermill": {
     "duration": 73.917774,
     "end_time": "2022-12-07T08:21:14.207407",
     "exception": false,
     "start_time": "2022-12-07T08:20:00.289633",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "tokenizer = v_tokenizer\n",
    "phobert = b.bert\n",
    "\n",
    "\n",
    "text_train = df_train['Comment'].to_list()\n",
    "label_train = df_train['Rating'].to_list()\n",
    "text_test = df_test['Comment'].to_list()\n",
    "label_test = df_test['Rating'].to_list()\n",
    "\n",
    "\n",
    "\n",
    "features_train = make_bert_features(text_train)\n",
    "features_test = make_bert_features(text_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ee5f7",
   "metadata": {
    "papermill": {
     "duration": 0.255248,
     "end_time": "2022-12-07T08:21:15.974099",
     "exception": false,
     "start_time": "2022-12-07T08:21:15.718851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## BERT Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4900b",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T08:21:16.483492Z",
     "iopub.status.busy": "2022-12-07T08:21:16.483116Z",
     "iopub.status.idle": "2022-12-07T08:21:16.488079Z",
     "shell.execute_reply": "2022-12-07T08:21:16.486882Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.262335,
     "end_time": "2022-12-07T08:21:16.490538",
     "exception": false,
     "start_time": "2022-12-07T08:21:16.228203",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def create_bert():\n",
    "#\n",
    "#     model = Sequential()\n",
    "#     model.add(embed)\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(768, use_bias=True))\n",
    "#     model.add(BatchNormalizationV1(500, epsilon=1e-5, momentum=0.1))\n",
    "#     model.add(ReLU())\n",
    "#     model.add(Dropout(0.1))\n",
    "#     model.add(Dense(500))\n",
    "#     #model.add(nn.Linear(in_features=768, out_features = 500, bias=True))\n",
    "#     # model.add(nn.BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True).to(device))\n",
    "#     # model.add(nn.ReLU(inplace=True))\n",
    "#     # model.add(nn.Dropout(p=0.1))\n",
    "#     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#     model.build(input_shape=(1,1,256,768))\n",
    "#     model.summary()\n",
    "#\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351750b1",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T08:21:17.051398Z",
     "iopub.status.busy": "2022-12-07T08:21:17.051031Z",
     "iopub.status.idle": "2022-12-07T08:21:17.055291Z",
     "shell.execute_reply": "2022-12-07T08:21:17.054388Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.311716,
     "end_time": "2022-12-07T08:21:17.057423",
     "exception": false,
     "start_time": "2022-12-07T08:21:16.745707",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bert = create_bert()\n",
    "# bert.built()\n",
    "# bert.fit(features_train, np.array(label_train))\n",
    "# predictions_bert = bert.predict(features_test)\n",
    "#\n",
    "# print(\"BERT Accuracy Score -> \",accuracy_score(predictions_bert, label_test)*100)\n",
    "# print(classification_report(label_test,predictions_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dca432",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-07T08:21:17.565536Z",
     "iopub.status.busy": "2022-12-07T08:21:17.564892Z",
     "iopub.status.idle": "2022-12-07T08:21:17.569573Z",
     "shell.execute_reply": "2022-12-07T08:21:17.568646Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.259846,
     "end_time": "2022-12-07T08:21:17.571467",
     "exception": false,
     "start_time": "2022-12-07T08:21:17.311621",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "#\n",
    "# step_factor = 0.02\n",
    "# threshold_value = 0.2\n",
    "# roc_score=0\n",
    "# predicted_proba = bert.predict_proba(features_test) #probability of prediction\n",
    "# while threshold_value <=0.9: #continue to check best threshold upto probability 0.8\n",
    "#     temp_thresh = threshold_value\n",
    "#     predicted = (predicted_proba [:,1] >= temp_thresh).astype('int') #change the class boundary for prediction\n",
    "#     print('Threshold',temp_thresh,'--',roc_auc_score(label_test, predicted))\n",
    "#     if roc_score<roc_auc_score(label_test, predicted): #store the threshold for best classification\n",
    "#         roc_score = roc_auc_score(label_test, predicted)\n",
    "#         thrsh_score = threshold_value\n",
    "#     threshold_value = threshold_value + step_factor\n",
    "# print('---Optimum Threshold ---',thrsh_score,'--ROC--',roc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9cbf8",
   "metadata": {
    "papermill": {
     "duration": 0.253565,
     "end_time": "2022-12-07T08:21:18.079887",
     "exception": false,
     "start_time": "2022-12-07T08:21:17.826322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LIGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d7532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:21:18.636891Z",
     "iopub.status.busy": "2022-12-07T08:21:18.636522Z",
     "iopub.status.idle": "2022-12-07T08:21:36.676355Z",
     "shell.execute_reply": "2022-12-07T08:21:36.675394Z"
    },
    "papermill": {
     "duration": 18.340419,
     "end_time": "2022-12-07T08:21:36.679589",
     "exception": false,
     "start_time": "2022-12-07T08:21:18.339170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgbm = LGBMClassifier()\n",
    "train = lgbm.fit(features_train,label_train)\n",
    "predictions_lgbm = lgbm.predict(features_test)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"ligbm Accuracy Score -> \",accuracy_score(predictions_lgbm, label_test)*100)\n",
    "print(classification_report(label_test,predictions_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1988a10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:21:37.254094Z",
     "iopub.status.busy": "2022-12-07T08:21:37.253735Z",
     "iopub.status.idle": "2022-12-07T08:21:37.274063Z",
     "shell.execute_reply": "2022-12-07T08:21:37.273041Z"
    },
    "papermill": {
     "duration": 0.281964,
     "end_time": "2022-12-07T08:21:37.276224",
     "exception": false,
     "start_time": "2022-12-07T08:21:36.994260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "predictions_lgbm = (lgbm.predict_proba(features_test)[:,1] >= 0.5).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a25e568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:21:37.802684Z",
     "iopub.status.busy": "2022-12-07T08:21:37.802314Z",
     "iopub.status.idle": "2022-12-07T08:21:37.819780Z",
     "shell.execute_reply": "2022-12-07T08:21:37.818371Z"
    },
    "papermill": {
     "duration": 0.287804,
     "end_time": "2022-12-07T08:21:37.821673",
     "exception": false,
     "start_time": "2022-12-07T08:21:37.533869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"ligbm Accuracy Score -> \",accuracy_score(predictions_lgbm, label_test)*100)\n",
    "print(classification_report(label_test,predictions_lgbm))\n",
    "print(roc_auc_score(label_test, predictions_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53796327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:21:38.385434Z",
     "iopub.status.busy": "2022-12-07T08:21:38.385070Z",
     "iopub.status.idle": "2022-12-07T08:21:38.389674Z",
     "shell.execute_reply": "2022-12-07T08:21:38.388597Z"
    },
    "papermill": {
     "duration": 0.264798,
     "end_time": "2022-12-07T08:21:38.391660",
     "exception": false,
     "start_time": "2022-12-07T08:21:38.126862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "# # boosting_type = 'goss', learning_rate = 0.05, n_estimators = 200,num_leaves = 20, scale_pos_weight = 0.7\n",
    "# classifier = BalancedBaggingClassifier(base_estimator=LGBMClassifier(),\n",
    "#                                 sampling_strategy='not majority',\n",
    "#                                 replacement=False,\n",
    "#                                 random_state=42)\n",
    "\n",
    "# classifier.fit(features_train,label_train)\n",
    "# predictions_bag = classifier.predict(features_test)\n",
    "# # Use accuracy_score function to get the accuracy\n",
    "# print(\"ligbm Accuracy Score -> \",accuracy_score(predictions_bag, label_test)*100)\n",
    "# print(classification_report(label_test,predictions_bag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ed78b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:21:38.909427Z",
     "iopub.status.busy": "2022-12-07T08:21:38.909017Z",
     "iopub.status.idle": "2022-12-07T08:21:39.016404Z",
     "shell.execute_reply": "2022-12-07T08:21:39.015254Z"
    },
    "papermill": {
     "duration": 0.370391,
     "end_time": "2022-12-07T08:21:39.018759",
     "exception": false,
     "start_time": "2022-12-07T08:21:38.648368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "step_factor = 0.02 \n",
    "threshold_value = 0.2 \n",
    "roc_score=0\n",
    "predicted_proba = lgbm.predict_proba(features_test) #probability of prediction\n",
    "while threshold_value <=0.9: #continue to check best threshold upto probability 0.8\n",
    "    temp_thresh = threshold_value\n",
    "    predicted = (predicted_proba [:,1] >= temp_thresh).astype('int') #change the class boundary for prediction\n",
    "    print('Threshold',temp_thresh,'--',roc_auc_score(label_test, predicted))\n",
    "    if roc_score<roc_auc_score(label_test, predicted): #store the threshold for best classification\n",
    "        roc_score = roc_auc_score(label_test, predicted)\n",
    "        thrsh_score = threshold_value\n",
    "    threshold_value = threshold_value + step_factor\n",
    "print('---Optimum Threshold ---',thrsh_score,'--ROC--',roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ee64f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:21:39.608211Z",
     "iopub.status.busy": "2022-12-07T08:21:39.607823Z",
     "iopub.status.idle": "2022-12-07T08:21:39.745455Z",
     "shell.execute_reply": "2022-12-07T08:21:39.743783Z"
    },
    "papermill": {
     "duration": 0.470092,
     "end_time": "2022-12-07T08:21:39.747785",
     "exception": false,
     "start_time": "2022-12-07T08:21:39.277693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_lgbm = lgbm.predict(features_train)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"ligbm Accuracy Score -> \",accuracy_score(predictions_lgbm, label_train)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cba9b7",
   "metadata": {
    "papermill": {
     "duration": 0.255824,
     "end_time": "2022-12-07T08:21:40.267176",
     "exception": false,
     "start_time": "2022-12-07T08:21:40.011352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d210fd10",
   "metadata": {
    "papermill": {
     "duration": 0.255697,
     "end_time": "2022-12-07T08:21:40.779614",
     "exception": false,
     "start_time": "2022-12-07T08:21:40.523917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ADA boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93797a6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:21:41.343278Z",
     "iopub.status.busy": "2022-12-07T08:21:41.342905Z",
     "iopub.status.idle": "2022-12-07T08:24:18.517846Z",
     "shell.execute_reply": "2022-12-07T08:24:18.516740Z"
    },
    "papermill": {
     "duration": 157.69192,
     "end_time": "2022-12-07T08:24:18.774904",
     "exception": false,
     "start_time": "2022-12-07T08:21:41.082984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(features_train,label_train)\n",
    "\n",
    "predictions_ada = clf.predict(features_test)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Ada Accuracy Score -> \",accuracy_score(predictions_ada, label_test)*100)\n",
    "print(classification_report(label_test,predictions_ada))\n",
    "\n",
    "joblib.dump(clf, \"ada.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc776faf",
   "metadata": {
    "papermill": {
     "duration": 0.259457,
     "end_time": "2022-12-07T08:24:19.289544",
     "exception": false,
     "start_time": "2022-12-07T08:24:19.030087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5251782",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:24:19.850848Z",
     "iopub.status.busy": "2022-12-07T08:24:19.850190Z",
     "iopub.status.idle": "2022-12-07T08:25:07.933003Z",
     "shell.execute_reply": "2022-12-07T08:25:07.931888Z"
    },
    "papermill": {
     "duration": 48.633461,
     "end_time": "2022-12-07T08:25:08.178663",
     "exception": false,
     "start_time": "2022-12-07T08:24:19.545202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(features_train,label_train)\n",
    "predictions_rf = rf.predict(features_test)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"rf Accuracy Score -> \",accuracy_score(predictions_rf, label_test)*100)\n",
    "print(classification_report(label_test,predictions_rf))\n",
    "\n",
    "joblib.dump(rf, \"rf.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d58bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:25:08.687667Z",
     "iopub.status.busy": "2022-12-07T08:25:08.687248Z",
     "iopub.status.idle": "2022-12-07T08:25:08.813894Z",
     "shell.execute_reply": "2022-12-07T08:25:08.812809Z"
    },
    "papermill": {
     "duration": 0.382906,
     "end_time": "2022-12-07T08:25:08.816309",
     "exception": false,
     "start_time": "2022-12-07T08:25:08.433403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_rf = rf.predict(features_test)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"rf Accuracy Score -> \",accuracy_score(predictions_rf, label_test)*100)\n",
    "print(classification_report(label_test,predictions_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d14b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:25:09.331012Z",
     "iopub.status.busy": "2022-12-07T08:25:09.330659Z",
     "iopub.status.idle": "2022-12-07T08:25:09.540782Z",
     "shell.execute_reply": "2022-12-07T08:25:09.539312Z"
    },
    "papermill": {
     "duration": 0.469325,
     "end_time": "2022-12-07T08:25:09.543108",
     "exception": false,
     "start_time": "2022-12-07T08:25:09.073783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_factor = 0.02 \n",
    "threshold_value = 0.2 \n",
    "roc_score=0\n",
    "predicted_proba = rf.predict_proba(features_test) #probability of prediction\n",
    "while threshold_value <=0.9: #continue to check best threshold upto probability 0.8\n",
    "    temp_thresh = threshold_value\n",
    "    predicted = (predicted_proba [:,1] >= temp_thresh).astype('int') #change the class boundary for prediction\n",
    "    print('Threshold',temp_thresh,'--',roc_auc_score(label_test, predicted))\n",
    "    if roc_score<roc_auc_score(label_test, predicted): #store the threshold for best classification\n",
    "        roc_score = roc_auc_score(label_test, predicted)\n",
    "        thrsh_score = threshold_value\n",
    "    threshold_value = threshold_value + step_factor\n",
    "print('---Optimum Threshold ---',thrsh_score,'--ROC--',roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22856518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:25:10.104557Z",
     "iopub.status.busy": "2022-12-07T08:25:10.103515Z",
     "iopub.status.idle": "2022-12-07T08:25:10.108354Z",
     "shell.execute_reply": "2022-12-07T08:25:10.107388Z"
    },
    "papermill": {
     "duration": 0.260563,
     "end_time": "2022-12-07T08:25:10.110192",
     "exception": false,
     "start_time": "2022-12-07T08:25:09.849629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_test['predicted_label'] = predictions_rf\n",
    "# df_test.to_csv('test_predict.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97238d9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:25:10.619979Z",
     "iopub.status.busy": "2022-12-07T08:25:10.619632Z",
     "iopub.status.idle": "2022-12-07T08:25:10.623912Z",
     "shell.execute_reply": "2022-12-07T08:25:10.622921Z"
    },
    "papermill": {
     "duration": 0.260757,
     "end_time": "2022-12-07T08:25:10.626014",
     "exception": false,
     "start_time": "2022-12-07T08:25:10.365257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e532b80",
   "metadata": {
    "papermill": {
     "duration": 0.298713,
     "end_time": "2022-12-07T08:25:11.180241",
     "exception": false,
     "start_time": "2022-12-07T08:25:10.881528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2997ab59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:25:11.691771Z",
     "iopub.status.busy": "2022-12-07T08:25:11.691421Z",
     "iopub.status.idle": "2022-12-07T08:25:17.119319Z",
     "shell.execute_reply": "2022-12-07T08:25:17.118205Z"
    },
    "papermill": {
     "duration": 5.684527,
     "end_time": "2022-12-07T08:25:17.121270",
     "exception": false,
     "start_time": "2022-12-07T08:25:11.436743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb = xgb.XGBClassifier()\n",
    "xgb.fit(features_train,label_train)\n",
    "y_train_predict = xgb.predict(features_train)\n",
    "print(\"XGB Accuracy Score of train set-> \",accuracy_score(label_train, y_train_predict)*100)\n",
    "\n",
    "y_test_predict = xgb.predict(features_test)\n",
    "print(\"xgb Accuracy Score -> \",accuracy_score(label_test,y_test_predict)*100)\n",
    "print(classification_report(label_test,y_test_predict))\n",
    "\n",
    "joblib.dump(xgb, \"xgb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd7df5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:25:17.642726Z",
     "iopub.status.busy": "2022-12-07T08:25:17.642362Z",
     "iopub.status.idle": "2022-12-07T08:25:17.649319Z",
     "shell.execute_reply": "2022-12-07T08:25:17.648228Z"
    },
    "papermill": {
     "duration": 0.266551,
     "end_time": "2022-12-07T08:25:17.652538",
     "exception": false,
     "start_time": "2022-12-07T08:25:17.385987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"xgb Accuracy Score -> \",accuracy_score(label_test,y_test_predict)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CAT BOOST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34421b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:25:18.216166Z",
     "iopub.status.busy": "2022-12-07T08:25:18.215796Z",
     "iopub.status.idle": "2022-12-07T08:25:18.220744Z",
     "shell.execute_reply": "2022-12-07T08:25:18.219765Z"
    },
    "papermill": {
     "duration": 0.311418,
     "end_time": "2022-12-07T08:25:18.222892",
     "exception": false,
     "start_time": "2022-12-07T08:25:17.911474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "# clf = CatBoostClassifier(task_type = \"GPU\", iterations = 1100,depth = 15)\n",
    "#\n",
    "# # ,iterations=1000,learning_rate = 0.15,depth =10)\n",
    "# # , ,cat_features = LIST_FEAT1 + LIST_FEAT2 + LIST_FEAT5 + LIST_FEAT6 + LIST_FEAT7 + LIST_FEAT8)\n",
    "# clf.fit(features_train,label_train)\n",
    "# y_train_predict = clf.predict(features_train)\n",
    "# print(\"CBoost Accuracy Score of train set-> \",accuracy_score(label_train, y_train_predict)*100)\n",
    "#\n",
    "# y_test_predict = clf.predict(features_test)\n",
    "# print(classification_report(label_test,y_test_predict))\n",
    "#\n",
    "# joblib.dump(clf, \"catboost.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe827f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:25:18.734054Z",
     "iopub.status.busy": "2022-12-07T08:25:18.733708Z",
     "iopub.status.idle": "2022-12-07T08:25:26.186735Z",
     "shell.execute_reply": "2022-12-07T08:25:26.185712Z"
    },
    "papermill": {
     "duration": 7.712602,
     "end_time": "2022-12-07T08:25:26.189817",
     "exception": false,
     "start_time": "2022-12-07T08:25:18.477215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SVM = SVC()\n",
    "SVM.fit(features_train,label_train)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(features_test)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, label_test)*100)\n",
    "\n",
    "joblib.dump(SVM, \"svm.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e151e2a",
   "metadata": {
    "papermill": {
     "duration": 0.43702,
     "end_time": "2022-12-07T08:25:27.094474",
     "exception": false,
     "start_time": "2022-12-07T08:25:26.657454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb91e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:25:27.922137Z",
     "iopub.status.busy": "2022-12-07T08:25:27.921499Z",
     "iopub.status.idle": "2022-12-07T08:25:29.980727Z",
     "shell.execute_reply": "2022-12-07T08:25:29.977589Z"
    },
    "papermill": {
     "duration": 2.449527,
     "end_time": "2022-12-07T08:25:29.984609",
     "exception": false,
     "start_time": "2022-12-07T08:25:27.535082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "reg = LogisticRegression()\n",
    "\n",
    "reg.fit(features_train,label_train)\n",
    "y_train_predict = reg.predict(features_train)\n",
    "print(\"LG Accuracy Score of train set-> \",accuracy_score(label_train, y_train_predict)*100)\n",
    "\n",
    "y_test_predict = reg.predict(features_test)\n",
    "print(\"LG Accuracy Score -> \",accuracy_score(label_test,y_test_predict)*100)\n",
    "print(classification_report(label_test,y_test_predict))\n",
    "\n",
    "joblib.dump(reg, \"lg.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbded9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:25:30.593705Z",
     "iopub.status.busy": "2022-12-07T08:25:30.593311Z",
     "iopub.status.idle": "2022-12-07T08:25:30.625182Z",
     "shell.execute_reply": "2022-12-07T08:25:30.623710Z"
    },
    "papermill": {
     "duration": 0.308409,
     "end_time": "2022-12-07T08:25:30.629543",
     "exception": false,
     "start_time": "2022-12-07T08:25:30.321134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_vt = (reg.predict_proba(features_test)[:,1] >= 0.51).astype(bool)\n",
    "\n",
    "print(\"LG Accuracy Score -> \",accuracy_score(predictions_vt, label_test)*100)\n",
    "print(classification_report(label_test,predictions_vt))\n",
    "print(roc_auc_score(label_test, predictions_vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9ef021",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:25:31.244438Z",
     "iopub.status.busy": "2022-12-07T08:25:31.243997Z",
     "iopub.status.idle": "2022-12-07T08:25:31.402440Z",
     "shell.execute_reply": "2022-12-07T08:25:31.401109Z"
    },
    "papermill": {
     "duration": 0.429695,
     "end_time": "2022-12-07T08:25:31.405103",
     "exception": false,
     "start_time": "2022-12-07T08:25:30.975408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_factor = 0.02 \n",
    "threshold_value = 0.2 \n",
    "roc_score=0\n",
    "predicted_proba = reg.predict_proba(features_test) #probability of prediction\n",
    "while threshold_value <=0.8: #continue to check best threshold upto probability 0.8\n",
    "    temp_thresh = threshold_value\n",
    "    predicted = (predicted_proba [:,1] >= temp_thresh).astype('int') #change the class boundary for prediction\n",
    "    print('Threshold',temp_thresh,'--',roc_auc_score(label_test, predicted))\n",
    "    if roc_score<roc_auc_score(label_test, predicted): #store the threshold for best classification\n",
    "        roc_score = roc_auc_score(label_test, predicted)\n",
    "        thrsh_score = threshold_value\n",
    "    threshold_value = threshold_value + step_factor\n",
    "print('---Optimum Threshold ---',thrsh_score,'--ROC--',roc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MLP Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(activation = \"logistic\", alpha = 0.1, hidden_layer_sizes = (10, 10, 10),\n",
    "                            learning_rate = \"constant\", max_iter = 300, random_state = 42)\n",
    "\n",
    "mlp.fit(features_train,label_train)\n",
    "y_train_predict = mlp.predict(features_train)\n",
    "print(\"MLP Accuracy Score of train set-> \",accuracy_score(label_train, y_train_predict)*100)\n",
    "\n",
    "y_test_predict = mlp.predict(features_test)\n",
    "print(\"MLP Accuracy Score -> \",accuracy_score(label_test,y_test_predict)*100)\n",
    "print(classification_report(label_test,y_test_predict))\n",
    "\n",
    "joblib.dump(reg, \"mlp.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NuSVC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVC\n",
    "\n",
    "nu_svc = NuSVC(degree = 1, kernel = \"linear\", nu = 0.1, probability = True, max_iter=300)\n",
    "\n",
    "nu_svc.fit(features_train,label_train)\n",
    "y_train_predict = nu_svc.predict(features_train)\n",
    "print(\"NuSVC Accuracy Score of train set-> \",accuracy_score(label_train, y_train_predict)*100)\n",
    "\n",
    "y_test_predict = nu_svc.predict(features_test)\n",
    "print(\"NuSVC Accuracy Score -> \",accuracy_score(label_test,y_test_predict)*100)\n",
    "print(classification_report(label_test,y_test_predict))\n",
    "\n",
    "joblib.dump(reg, \"nusvc.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Building the model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    embed = Embedding(input_dim = 60000, output_dim = 20, input_length = features_train.shape[1])\n",
    "    model.add(embed)\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(200, return_sequences = True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(200, return_sequences = True)))\n",
    "    model.add(AttentionWithContext())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "09939845",
   "metadata": {
    "papermill": {
     "duration": 0.263482,
     "end_time": "2022-12-07T08:25:32.527514",
     "exception": false,
     "start_time": "2022-12-07T08:25:32.264032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df11c5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:25:33.097676Z",
     "iopub.status.busy": "2022-12-07T08:25:33.097295Z",
     "iopub.status.idle": "2022-12-07T08:29:21.300461Z",
     "shell.execute_reply": "2022-12-07T08:29:21.299496Z"
    },
    "papermill": {
     "duration": 228.767439,
     "end_time": "2022-12-07T08:29:21.561538",
     "exception": false,
     "start_time": "2022-12-07T08:25:32.794099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from keras.models import load_model\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500,min_samples_leaf=2,\n",
    "                                min_samples_split= 10, max_features = 'sqrt', criterion = 'entropy', bootstrap= True,\n",
    "                              random_state=42)\n",
    "lgbm = LGBMClassifier(boosting_type = 'goss', learning_rate = 0.05, n_estimators = 200,num_leaves = 20, scale_pos_weight = 0.7)\n",
    "lg = LogisticRegression(max_iter=200)\n",
    "xgb = XGBClassifier(n_estimators = 200, max_depth=20,tree_method=\"hist\")\n",
    "svm = SVC(kernel='linear', probability=True, gamma=0.2, degree=1, verbose=True, tol=1e-5)\n",
    "ada = AdaBoostClassifier(n_estimators=200, random_state=1, learning_rate=1e-5)\n",
    "mlp = MLPClassifier(activation = \"relu\", alpha = 0.1, hidden_layer_sizes = (10,10,10),\n",
    "                            learning_rate = \"constant\", max_iter = 2000, random_state = 1000)\n",
    "nu_svc = NuSVC(degree = 1, kernel = \"rbf\", nu = 0.25, probability = True)\n",
    "\n",
    "rf = joblib.load(\"rf.pkl\")\n",
    "lg = joblib.load(\"lg.pkl\")\n",
    "xgb = joblib.load(\"xgb.pkl\")\n",
    "svm = joblib.load(\"svm.pkl\")\n",
    "ada = joblib.load(\"ada.pkl\")\n",
    "mlp = joblib.load(\"mlp.pkl\")\n",
    "nu_svc = joblib.load(\"nusvc.pkl\")\n",
    "\n",
    "lstm = KerasClassifier(build_fn= lambda: create_model())\n",
    "lstm._estimator_type = \"classifier\"\n",
    "\n",
    "# bert = KerasClassifier(build_fn = lambda: create_bert())\n",
    "# bert._estimator_type = 'classifier'\n",
    "\n",
    "estimator = []\n",
    "# estimator.append(('LR', \n",
    "#                   LogisticRegression(solver ='lbfgs', \n",
    "#                                      multi_class ='multinomial', \n",
    "#                                      max_iter = 200)))\n",
    "\n",
    "estimator.append(('RF', rf))\n",
    "estimator.append(('XGB', xgb))\n",
    "estimator.append(('LGBM', lgbm))\n",
    "# estimator.append(('MLP', mlp))\n",
    "# estimator.append(('SVM', svm ))\n",
    "# estimator.append(('NuSVC', nu_svc))\n",
    "estimator.append(('LG', lg))\n",
    "# estimator.append(('LSTM', lstm))\n",
    "estimator.append(('ADA', ada))\n",
    "# estimator.append(('BERT', bert))\n",
    "# estimator.append(('LSTM', lstm))\n",
    "\n",
    "# voting ='soft'\n",
    "vot_hard = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "vot_hard.fit(features_train,label_train)\n",
    "y_pred = vot_hard.predict(features_test)\n",
    "                 \n",
    "print(\"Voting Accuracy Score -> \",accuracy_score(y_pred, label_test)*100)\n",
    "\n",
    "print(classification_report(label_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649cd2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:29:22.072985Z",
     "iopub.status.busy": "2022-12-07T08:29:22.072277Z",
     "iopub.status.idle": "2022-12-07T08:29:22.471190Z",
     "shell.execute_reply": "2022-12-07T08:29:22.470106Z"
    },
    "papermill": {
     "duration": 0.658785,
     "end_time": "2022-12-07T08:29:22.473599",
     "exception": false,
     "start_time": "2022-12-07T08:29:21.814814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_vt = (vot_hard.predict_proba(features_test)[:,1] >= 0.47).astype(bool)\n",
    "\n",
    "print(\"voting Accuracy Score -> \",accuracy_score(predictions_vt, label_test)*100)\n",
    "print(classification_report(label_test,predictions_vt))\n",
    "print(roc_auc_score(label_test, predictions_vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38123c5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:29:23.036346Z",
     "iopub.status.busy": "2022-12-07T08:29:23.035974Z",
     "iopub.status.idle": "2022-12-07T08:29:23.520420Z",
     "shell.execute_reply": "2022-12-07T08:29:23.519400Z"
    },
    "papermill": {
     "duration": 0.792929,
     "end_time": "2022-12-07T08:29:23.522465",
     "exception": false,
     "start_time": "2022-12-07T08:29:22.729536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_factor = 0.02 \n",
    "threshold_value = 0.2 \n",
    "roc_score=0\n",
    "predicted_proba = vot_hard.predict_proba(features_test) #probability of prediction\n",
    "while threshold_value <=0.9: #continue to check best threshold upto probability 0.8\n",
    "    temp_thresh = threshold_value\n",
    "    predicted = (predicted_proba [:,1] >= temp_thresh).astype('int') #change the class boundary for prediction\n",
    "    print('Threshold',temp_thresh,'--',roc_auc_score(label_test, predicted))\n",
    "    if roc_score<roc_auc_score(label_test, predicted): #store the threshold for best classification\n",
    "        roc_score = roc_auc_score(label_test, predicted)\n",
    "        thrsh_score = threshold_value\n",
    "    threshold_value = threshold_value + step_factor\n",
    "print('---Optimum Threshold ---',thrsh_score,'--ROC--',roc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a75e4",
   "metadata": {
    "papermill": {
     "duration": 0.256165,
     "end_time": "2022-12-07T08:29:24.035386",
     "exception": false,
     "start_time": "2022-12-07T08:29:23.779221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e3a66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:29:24.554930Z",
     "iopub.status.busy": "2022-12-07T08:29:24.554000Z",
     "iopub.status.idle": "2022-12-07T08:35:05.944520Z",
     "shell.execute_reply": "2022-12-07T08:35:05.943539Z"
    },
    "papermill": {
     "duration": 341.918198,
     "end_time": "2022-12-07T08:35:06.209130",
     "exception": false,
     "start_time": "2022-12-07T08:29:24.290932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "estimators = [\n",
    "    # ('RF', rf),\n",
    "    # ('XGB', xgb),\n",
    "    # ('LGBM', lgbm),\n",
    "    # ('SVM', svm),\n",
    "    (\"RF\", rf),\n",
    "    ('XGB', xgb),\n",
    "    ('LGBM', lgbm),\n",
    "    # ('LSTM', lstm),\n",
    "    # ('NuSVC', nu_svc)\n",
    "]\n",
    "\n",
    "st = StackingClassifier(estimators=estimators, final_estimator = ada)\n",
    "st.fit(features_train,label_train)\n",
    "y_pred = st.predict(features_test)\n",
    "                 \n",
    "print(\"ST Accuracy Score -> \",accuracy_score(y_pred, label_test)*100)\n",
    "\n",
    "print(classification_report(label_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5516c00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:35:06.769082Z",
     "iopub.status.busy": "2022-12-07T08:35:06.768710Z",
     "iopub.status.idle": "2022-12-07T08:35:06.940043Z",
     "shell.execute_reply": "2022-12-07T08:35:06.938712Z"
    },
    "papermill": {
     "duration": 0.432692,
     "end_time": "2022-12-07T08:35:06.942213",
     "exception": false,
     "start_time": "2022-12-07T08:35:06.509521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_st = (st.predict_proba(features_test)[:,1] >= 0.5).astype(bool)\n",
    "\n",
    "print(\"ST Accuracy Score -> \",accuracy_score(predictions_st, label_test)*100)\n",
    "print(classification_report(label_test,predictions_st))\n",
    "print(roc_auc_score(label_test, predictions_st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52c6bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:35:07.458410Z",
     "iopub.status.busy": "2022-12-07T08:35:07.457291Z",
     "iopub.status.idle": "2022-12-07T08:35:07.694639Z",
     "shell.execute_reply": "2022-12-07T08:35:07.693297Z"
    },
    "papermill": {
     "duration": 0.496468,
     "end_time": "2022-12-07T08:35:07.696813",
     "exception": false,
     "start_time": "2022-12-07T08:35:07.200345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "step_factor = 0.02 \n",
    "threshold_value = 0.2 \n",
    "roc_score=0\n",
    "predicted_proba = st.predict_proba(features_test) #probability of prediction\n",
    "while threshold_value <=0.8: #continue to check best threshold upto probability 0.8\n",
    "    temp_thresh = threshold_value\n",
    "    predicted = (predicted_proba [:,1] >= temp_thresh).astype('int') #change the class boundary for prediction\n",
    "    print('Threshold',temp_thresh,'--',roc_auc_score(label_test, predicted))\n",
    "    if roc_score<roc_auc_score(label_test, predicted): #store the threshold for best classification\n",
    "        roc_score = roc_auc_score(label_test, predicted)\n",
    "        thrsh_score = threshold_value\n",
    "    threshold_value = threshold_value + step_factor\n",
    "print('---Optimum Threshold ---',thrsh_score,'--ROC--',roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf45b21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:35:08.264440Z",
     "iopub.status.busy": "2022-12-07T08:35:08.263442Z",
     "iopub.status.idle": "2022-12-07T08:35:08.273040Z",
     "shell.execute_reply": "2022-12-07T08:35:08.272229Z"
    },
    "papermill": {
     "duration": 0.316369,
     "end_time": "2022-12-07T08:35:08.274955",
     "exception": false,
     "start_time": "2022-12-07T08:35:07.958586",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f203a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:35:08.790514Z",
     "iopub.status.busy": "2022-12-07T08:35:08.789545Z",
     "iopub.status.idle": "2022-12-07T08:35:08.896061Z",
     "shell.execute_reply": "2022-12-07T08:35:08.895109Z"
    },
    "papermill": {
     "duration": 0.368256,
     "end_time": "2022-12-07T08:35:08.898415",
     "exception": false,
     "start_time": "2022-12-07T08:35:08.530159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submit = pd.read_csv('Dataset/vuong_test.csv')\n",
    "select_indices = list(np.where(df_submit.loc[:,\"Comment\"].isnull()))\n",
    "for item in select_indices[0]: #select_indices is an array, you need to access the first element to print the indexes\n",
    "    print(item)\n",
    "df_submit = df_submit.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae841a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:35:09.418289Z",
     "iopub.status.busy": "2022-12-07T08:35:09.417894Z",
     "iopub.status.idle": "2022-12-07T08:35:51.782835Z",
     "shell.execute_reply": "2022-12-07T08:35:51.781406Z"
    },
    "papermill": {
     "duration": 42.628004,
     "end_time": "2022-12-07T08:35:51.785092",
     "exception": false,
     "start_time": "2022-12-07T08:35:09.157088",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_sub = df_submit['Comment'].to_list()\n",
    "features_sub = make_bert_features(text_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d980332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:35:52.360837Z",
     "iopub.status.busy": "2022-12-07T08:35:52.360484Z",
     "iopub.status.idle": "2022-12-07T08:35:53.665866Z",
     "shell.execute_reply": "2022-12-07T08:35:53.664852Z"
    },
    "papermill": {
     "duration": 1.619242,
     "end_time": "2022-12-07T08:35:53.668999",
     "exception": false,
     "start_time": "2022-12-07T08:35:52.049757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_sub = vot_hard.predict_proba(features_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d3f61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:35:54.643153Z",
     "iopub.status.busy": "2022-12-07T08:35:54.642697Z",
     "iopub.status.idle": "2022-12-07T08:35:54.648255Z",
     "shell.execute_reply": "2022-12-07T08:35:54.647403Z"
    },
    "papermill": {
     "duration": 0.569924,
     "end_time": "2022-12-07T08:35:54.652702",
     "exception": false,
     "start_time": "2022-12-07T08:35:54.082778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submit['Rating'] = predicted_sub[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8555676c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:35:55.321126Z",
     "iopub.status.busy": "2022-12-07T08:35:55.320779Z",
     "iopub.status.idle": "2022-12-07T08:35:55.327477Z",
     "shell.execute_reply": "2022-12-07T08:35:55.326538Z"
    },
    "papermill": {
     "duration": 0.287574,
     "end_time": "2022-12-07T08:35:55.329974",
     "exception": false,
     "start_time": "2022-12-07T08:35:55.042400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submit['Comment'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427c070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:35:55.915953Z",
     "iopub.status.busy": "2022-12-07T08:35:55.915023Z",
     "iopub.status.idle": "2022-12-07T08:35:55.939535Z",
     "shell.execute_reply": "2022-12-07T08:35:55.938604Z"
    },
    "papermill": {
     "duration": 0.29268,
     "end_time": "2022-12-07T08:35:55.941687",
     "exception": false,
     "start_time": "2022-12-07T08:35:55.649007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_submit[['RevId','Rating']].to_csv('submit_bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80783a3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T08:35:56.475218Z",
     "iopub.status.busy": "2022-12-07T08:35:56.474872Z",
     "iopub.status.idle": "2022-12-07T08:36:06.092544Z",
     "shell.execute_reply": "2022-12-07T08:36:06.091402Z"
    },
    "papermill": {
     "duration": 9.887729,
     "end_time": "2022-12-07T08:36:06.095012",
     "exception": false,
     "start_time": "2022-12-07T08:35:56.207283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "\n",
    "net = b\n",
    "net.eval()\n",
    "Data_test, attention_test = b.tokenize(text_test, label_test)\n",
    "attention = attention_test\n",
    "\n",
    "list_pred = np.array([])\n",
    "            \n",
    "for (inputs, labels), i_attention in tqdm(zip(Data_test, attention)):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    i_attention = i_attention.to(device)\n",
    "    \n",
    "    outputs = b(inputs,i_attention)\n",
    "        \n",
    "    loss = b.criterior(outputs,labels)      \n",
    "        \n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "#     print(preds.cpu().numpy())\n",
    "    list_pred = np.append(list_pred,preds.cpu().numpy())\n",
    "        \n",
    "    epoch_loss += loss.item() * inputs.size(0)\n",
    "    epoch_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "epoch_loss = epoch_loss / len(Data_test.dataset)\n",
    "epoch_acc = epoch_corrects.double() / len(Data_test.dataset)\n",
    "            \n",
    "print(\" Loss: {:.4f}  Acc: {:.4f}\".format( epoch_loss, epoch_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2301.867294,
   "end_time": "2022-12-07T08:36:09.631582",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-07T07:57:47.764288",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "012c40b9d45b4883bff071deb66d6d7f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_48387a96d315406389c0f096fe514d4c",
       "placeholder": "​",
       "style": "IPY_MODEL_496c9476f10541d59d95611865a401f9",
       "value": "Downloading: 100%"
      }
     },
     "041b902ab089499396b3f7ce0f2ee2c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3d8b7532515645699693cb1aa9465d22",
       "placeholder": "​",
       "style": "IPY_MODEL_3c65fb5d35db4f44afcc78f474adb4ef",
       "value": " 874k/874k [00:00&lt;00:00, 3.64MB/s]"
      }
     },
     "06f2dc23939b44c8a3b461a3d16965f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dfc553505c92422995049657e4e5ae6e",
       "placeholder": "​",
       "style": "IPY_MODEL_c44c6d12b4f44a33a234b5b81888d034",
       "value": "Downloading: 100%"
      }
     },
     "07f18fd193984182b9d6372f48c94e55": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "11c85a8802cc4a37b8fe07bc888739d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13954e34cbd5453580579b55d130d0e2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13cf6d734c614063b1d5560747bd2ac9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18900ed43a914328b9e5e589ce377844": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_07f18fd193984182b9d6372f48c94e55",
       "max": 1135173.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3f8ee00a8eeb4923a198efc48e253ef5",
       "value": 1135173.0
      }
     },
     "32df4be713b44162a5f6dac267f10239": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_11c85a8802cc4a37b8fe07bc888739d8",
       "placeholder": "​",
       "style": "IPY_MODEL_83d87ecdb7af4b8793957d3a7170b6ca",
       "value": " 557/557 [00:00&lt;00:00, 10.9kB/s]"
      }
     },
     "331f5a2de240419486a2ef4b7d072cf5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3c65fb5d35db4f44afcc78f474adb4ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3d80d2c0d25f47c3b09baf2f27e43843": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3d8b7532515645699693cb1aa9465d22": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3f8ee00a8eeb4923a198efc48e253ef5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "48387a96d315406389c0f096fe514d4c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "496c9476f10541d59d95611865a401f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "49c94bd2d7e242baa7ce161431cf91db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_06f2dc23939b44c8a3b461a3d16965f5",
        "IPY_MODEL_18900ed43a914328b9e5e589ce377844",
        "IPY_MODEL_6dea0b339519455595a71536804cba1f"
       ],
       "layout": "IPY_MODEL_a225c49111ff49b09d21b7b3d2b05b7b"
      }
     },
     "4e2f9faf72ee478b9964c2d92c18145b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_13cf6d734c614063b1d5560747bd2ac9",
       "max": 895321.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3d80d2c0d25f47c3b09baf2f27e43843",
       "value": 895321.0
      }
     },
     "56966ee428764eaea65fdd8391e814a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5fd5f44f1f00443f81f6ec1612d681d3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6250ffa210f54ebfa94c00b3c6d33623": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9163f70ea35f40e0bd6ba03bec6e14e3",
        "IPY_MODEL_6ce219a0ba1d4caa9c3d6ee84d2ecd85",
        "IPY_MODEL_761fd94664064fbca6ad89ed0eb8a79f"
       ],
       "layout": "IPY_MODEL_b070b9b261144c20a4eb1bf9a6416d1c"
      }
     },
     "65d3a3b7c4774439bf2e6eac10228571": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_012c40b9d45b4883bff071deb66d6d7f",
        "IPY_MODEL_4e2f9faf72ee478b9964c2d92c18145b",
        "IPY_MODEL_041b902ab089499396b3f7ce0f2ee2c1"
       ],
       "layout": "IPY_MODEL_bc26017cfd754b03b0387d5735c0e64f"
      }
     },
     "6ce219a0ba1d4caa9c3d6ee84d2ecd85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_db3ff31e5e264bab877a93f1e0f5098a",
       "max": 5.42923308E8,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f50647e382ad484db8a4b4f1ecc58913",
       "value": 5.42923308E8
      }
     },
     "6dea0b339519455595a71536804cba1f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a03bb5c3a41d4c9b84a863d462aed40a",
       "placeholder": "​",
       "style": "IPY_MODEL_a38838fd47af49d198d09f1025379a01",
       "value": " 1.08M/1.08M [00:00&lt;00:00, 1.80MB/s]"
      }
     },
     "761fd94664064fbca6ad89ed0eb8a79f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7c0759896ddd485a989d502c296a9b10",
       "placeholder": "​",
       "style": "IPY_MODEL_cfee9286a90f4ca8b735957e98108342",
       "value": " 518M/518M [00:18&lt;00:00, 31.7MB/s]"
      }
     },
     "7c0759896ddd485a989d502c296a9b10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "83d87ecdb7af4b8793957d3a7170b6ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9163f70ea35f40e0bd6ba03bec6e14e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_56966ee428764eaea65fdd8391e814a4",
       "placeholder": "​",
       "style": "IPY_MODEL_331f5a2de240419486a2ef4b7d072cf5",
       "value": "Downloading: 100%"
      }
     },
     "a03bb5c3a41d4c9b84a863d462aed40a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a225c49111ff49b09d21b7b3d2b05b7b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a38838fd47af49d198d09f1025379a01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "aae61e1af47a452087762e8d0f9948b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_13954e34cbd5453580579b55d130d0e2",
       "placeholder": "​",
       "style": "IPY_MODEL_e44b59547e0d4c899887654684d9669c",
       "value": "Downloading: 100%"
      }
     },
     "b070b9b261144c20a4eb1bf9a6416d1c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bc26017cfd754b03b0387d5735c0e64f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c1c6d2bb64be49efb90ef91e007b7121": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_aae61e1af47a452087762e8d0f9948b0",
        "IPY_MODEL_c5199a059d904aeaa79723aef2f178ea",
        "IPY_MODEL_32df4be713b44162a5f6dac267f10239"
       ],
       "layout": "IPY_MODEL_5fd5f44f1f00443f81f6ec1612d681d3"
      }
     },
     "c44c6d12b4f44a33a234b5b81888d034": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c5199a059d904aeaa79723aef2f178ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e1521cc901c6490392df33af73955875",
       "max": 557.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e6dd7c5dfd4242e59dbd84396007fe17",
       "value": 557.0
      }
     },
     "cfee9286a90f4ca8b735957e98108342": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "db3ff31e5e264bab877a93f1e0f5098a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dfc553505c92422995049657e4e5ae6e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e1521cc901c6490392df33af73955875": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e44b59547e0d4c899887654684d9669c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e6dd7c5dfd4242e59dbd84396007fe17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f50647e382ad484db8a4b4f1ecc58913": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
